<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
  <title>Description of train_bp</title>
  <meta name="keywords" content="train_bp">
  <meta name="description" content="Creates a backpropagation neural network and trains it.">
  <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
  <meta name="generator" content="m2html &copy; 2003 Guillaume Flandin">
  <meta name="robots" content="index, follow">
  <link type="text/css" rel="stylesheet" href="../m2html.css">
</head>
<body>
<a name="_top"></a>
<!-- menu.html . -->
<h1>train_bp
</h1>

<h2><a name="_name"></a>PURPOSE <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
<div class="box"><strong>Creates a backpropagation neural network and trains it.</strong></div>

<h2><a name="_synopsis"></a>SYNOPSIS <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
<div class="box"><strong>function [bp] = train_bp(trainpats,traintargs,in_args) </strong></div>

<h2><a name="_description"></a>DESCRIPTION <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
<div class="fragment"><pre class="comment"> Creates a backpropagation neural network and trains it.

 [BP] = TRAIN_BP(TRAINPATS,TRAINTARGS,IN_ARGS)

 You need to call TEST_BP afterwards to assess how well this
 generalizes to the test data.

 See the distpat manual and the Mathworks Neural Networks manual
 for more information on backpropagation

 Requires the Mathworks Neural Networks toolbox -
 http://www.mathworks.com/products/neuralnet. See CLASS_BP_NETLAB.M
 if you want to call the freely-distributable Netlab backprop
 implementation instead of the Mathworks Neural Networks toolbox
 one

 PATS = nFeatures x nTimepoints
 TARGS = nOuts x nTimepoints

 BP contains all the other information that you might need when
 analysing the network's output, most of which is specific to
 backprop. Some of this information is redundantly stored in multiple
 places. This gets referred to as SCRATCHPAD outside this function

 The classifier functions use a IN_ARGS structure to store possible
 arguments (rather than a varargin and property/value pairs). This
 tends to be easier to manage when lots of arguments are
 involved. xxx

 IN_ARGS required fields:
 - nHidden - number of hidden units (0 for no hidden layer)

 IN_ARGS optional fields:
 - alg (default = 'traincgb'). The particular backpropagation
 algorithm to use

 - act_funct (default = 'logsig'). Activation function for each
 layer. Cell array with 1 or 2 cell strings set to 'purelin',
 'tansig' etc. See NN manual for more information

 - goal (default = 0.001). Stopping criterion - stop training when
 the mean squared error drops below this

 - epochs (default = 500). Stopping criterion - stop training
 after this many epochs

 - show (default = NaN). Change this to 25, for instance, to make it
 pop up a graph and text progress report every 25 epochs of training
 - intrusive

 This version is set up for ZeroOne normalized inputs alter newff
 function if this is not appropriate - xxx???</pre></div>

<!-- crossreference -->
<h2><a name="_cross"></a>CROSS-REFERENCE INFORMATION <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
This function calls:
<ul style="list-style-image:url(../matlabicon.gif)">
<li><a href="add_struct_fields.html" class="code" title="function [args] = add_struct_fields(specifieds,defaults)">add_struct_fields</a>	Auxiliary function</li></ul>
This function is called by:
<ul style="list-style-image:url(../matlabicon.gif)">
</ul>
<!-- crossreference -->

<h2><a name="_subfunctions"></a>SUBFUNCTIONS <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
<ul style="list-style-image:url(../matlabicon.gif)">
<li><a href="#_sub1" class="code">function [] = sanity_check(trainpats,traintargs,args)</a></li></ul>
<h2><a name="_source"></a>SOURCE CODE <a href="#_top"><img alt="^" border="0" src="../up.png"></a></h2>
<div class="fragment"><pre>0001 <a name="_sub0" href="#_subfunctions" class="code">function [bp] = train_bp(trainpats,traintargs,in_args)</a>
0002 
0003 <span class="comment">% Creates a backpropagation neural network and trains it.</span>
0004 <span class="comment">%</span>
0005 <span class="comment">% [BP] = TRAIN_BP(TRAINPATS,TRAINTARGS,IN_ARGS)</span>
0006 <span class="comment">%</span>
0007 <span class="comment">% You need to call TEST_BP afterwards to assess how well this</span>
0008 <span class="comment">% generalizes to the test data.</span>
0009 <span class="comment">%</span>
0010 <span class="comment">% See the distpat manual and the Mathworks Neural Networks manual</span>
0011 <span class="comment">% for more information on backpropagation</span>
0012 <span class="comment">%</span>
0013 <span class="comment">% Requires the Mathworks Neural Networks toolbox -</span>
0014 <span class="comment">% http://www.mathworks.com/products/neuralnet. See CLASS_BP_NETLAB.M</span>
0015 <span class="comment">% if you want to call the freely-distributable Netlab backprop</span>
0016 <span class="comment">% implementation instead of the Mathworks Neural Networks toolbox</span>
0017 <span class="comment">% one</span>
0018 <span class="comment">%</span>
0019 <span class="comment">% PATS = nFeatures x nTimepoints</span>
0020 <span class="comment">% TARGS = nOuts x nTimepoints</span>
0021 <span class="comment">%</span>
0022 <span class="comment">% BP contains all the other information that you might need when</span>
0023 <span class="comment">% analysing the network's output, most of which is specific to</span>
0024 <span class="comment">% backprop. Some of this information is redundantly stored in multiple</span>
0025 <span class="comment">% places. This gets referred to as SCRATCHPAD outside this function</span>
0026 <span class="comment">%</span>
0027 <span class="comment">% The classifier functions use a IN_ARGS structure to store possible</span>
0028 <span class="comment">% arguments (rather than a varargin and property/value pairs). This</span>
0029 <span class="comment">% tends to be easier to manage when lots of arguments are</span>
0030 <span class="comment">% involved. xxx</span>
0031 <span class="comment">%</span>
0032 <span class="comment">% IN_ARGS required fields:</span>
0033 <span class="comment">% - nHidden - number of hidden units (0 for no hidden layer)</span>
0034 <span class="comment">%</span>
0035 <span class="comment">% IN_ARGS optional fields:</span>
0036 <span class="comment">% - alg (default = 'traincgb'). The particular backpropagation</span>
0037 <span class="comment">% algorithm to use</span>
0038 <span class="comment">%</span>
0039 <span class="comment">% - act_funct (default = 'logsig'). Activation function for each</span>
0040 <span class="comment">% layer. Cell array with 1 or 2 cell strings set to 'purelin',</span>
0041 <span class="comment">% 'tansig' etc. See NN manual for more information</span>
0042 <span class="comment">%</span>
0043 <span class="comment">% - goal (default = 0.001). Stopping criterion - stop training when</span>
0044 <span class="comment">% the mean squared error drops below this</span>
0045 <span class="comment">%</span>
0046 <span class="comment">% - epochs (default = 500). Stopping criterion - stop training</span>
0047 <span class="comment">% after this many epochs</span>
0048 <span class="comment">%</span>
0049 <span class="comment">% - show (default = NaN). Change this to 25, for instance, to make it</span>
0050 <span class="comment">% pop up a graph and text progress report every 25 epochs of training</span>
0051 <span class="comment">% - intrusive</span>
0052 <span class="comment">%</span>
0053 <span class="comment">% This version is set up for ZeroOne normalized inputs alter newff</span>
0054 <span class="comment">% function if this is not appropriate - xxx???</span>
0055 
0056 
0057 
0058 <span class="comment">%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%</span>
0059 <span class="comment">% SORT ARGUMENTS</span>
0060 
0061 defaults.alg = <span class="string">'traincgb'</span>;
0062 defaults.act_funct{1} = <span class="string">'logsig'</span>;
0063 <span class="comment">% Need separate activation functions for each layer</span>
0064 <span class="keyword">if</span> in_args.nHidden
0065     defaults.act_funct{2} = <span class="string">'logsig'</span>;
0066 <span class="keyword">end</span>
0067 defaults.goal = 0.001;
0068 defaults.epochs = 500;
0069 defaults.show = NaN;
0070 
0071 <span class="comment">% Args contains the default args, unless the user has over-ridden them</span>
0072 args = <a href="add_struct_fields.html" class="code" title="function [args] = add_struct_fields(specifieds,defaults)">add_struct_fields</a>(in_args,defaults);
0073 bp.class_args = args;
0074 
0075 <a href="#_sub1" class="code" title="subfunction [] = sanity_check(trainpats,traintargs,args)">sanity_check</a>(trainpats,traintargs,args);
0076 
0077 
0078 
0079 <span class="comment">%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%</span>
0080 <span class="comment">% SETTING THINGS UP</span>
0081 
0082 bp.nOut = size(traintargs,1);
0083 
0084 <span class="comment">% Backprop needs to know the range of its input patterns xxx</span>
0085 patsminmax(:,1)=min(trainpats')'; 
0086 patsminmax(:,2)=max(trainpats')';
0087 
0088 
0089 
0090 <span class="comment">%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%</span>
0091 <span class="comment">% *** CREATING AND INITIALIZING THE NET ***</span>
0092 
0093 <span class="comment">% 2 layer BP (i.e. no hidden layer)</span>
0094 <span class="keyword">if</span> ~args.nHidden
0095   <span class="comment">% Initialize a feedforward net with nOut output units and</span>
0096   <span class="comment">% act_funct as the activation function</span>
0097   bp.net = newff(patsminmax,[bp.nOut],args.act_funct);
0098   <span class="comment">% Set every unit in the input layer to be fully connected to</span>
0099   <span class="comment">% every unit in the output layer</span>
0100   bp.net.outputConnect = [1]; <span class="comment">% 2-layer feedforward connectivity</span>
0101 
0102 <span class="comment">% 3 layer BP (i.e. with hidden layer)</span>
0103 <span class="keyword">else</span>
0104   <span class="comment">% Initialize as above, but setting both layers' activation</span>
0105   <span class="comment">% functions</span>
0106   bp.net = newff(patsminmax,[args.nHidden bp.nOut],bp.act_funct);
0107   <span class="comment">% Every input unit connected to every hidden unit, and every</span>
0108   <span class="comment">% hidden unit to every output unit</span>
0109   bp.net.outputConnect = [1 1];
0110 <span class="keyword">end</span> <span class="comment">% if 3 layer</span>
0111 
0112 bp.net = init(bp.net); <span class="comment">% initializes it</span>
0113 
0114 <span class="comment">% Setting the network's properties according to in_args</span>
0115 bp.net.trainFcn = args.alg;
0116 bp.net.trainParam.goal = args.goal;
0117 bp.net.trainParam.epochs = args.epochs;
0118 bp.net.trainParam.show = args.show; 
0119 
0120 
0121   
0122 <span class="comment">%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%</span>
0123 <span class="comment">% *** RUNNING THE NET ***</span>
0124 
0125 <span class="comment">% This is the main training function - see TRAIN.M in the NN toolbox</span>
0126 [bp.net, bp.training_record, bp.training_acts,bp.training_error]= <span class="keyword">...</span>
0127     train(bp.net,trainpats,traintargs);
0128 
0129 <span class="comment">% Note that these contain the activations for all the units (both</span>
0130 <span class="comment">% hidden and output). OUTIDX indexes just the output layer (whether</span>
0131 <span class="comment">% you have a hidden layer or not)</span>
0132 bp.outidx = [args.nHidden+1:args.nHidden+bp.nOut];
0133 
0134 
0135 
0136 <span class="comment">%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%</span>
0137 <a name="_sub1" href="#_subfunctions" class="code">function [] = sanity_check(trainpats,traintargs,args)</a>
0138 
0139 <span class="keyword">if</span> ~isfield(args,<span class="string">'nHidden'</span>)
0140   error(<span class="string">'Need an nHidden field'</span>);
0141 <span class="keyword">end</span>
0142 
0143 <span class="keyword">if</span> args.nHidden &lt; 0
0144   error(<span class="string">'Illegal number of hidden units'</span>);
0145 <span class="keyword">end</span>
0146 
0147 <span class="keyword">if</span> size(trainpats,2)==1
0148   error(<span class="string">'Can''t classify a single timepoint'</span>);
0149 <span class="keyword">end</span>
0150 
0151 <span class="keyword">if</span> size(trainpats,2) ~= size(traintargs,2)
0152   error(<span class="string">'Different number of training pats and targs timepoints'</span>);
0153 <span class="keyword">end</span>
0154</pre></div>
<hr><address>Generated on Wed 31-Aug-2005 15:27:57 by <strong><a href="http://www.artefact.tk/software/matlab/m2html/" target="_parent">m2html</a></strong> &copy; 2003</address>
</body>
</html>